# gpu-batch-benchmark.yaml
AWSTemplateFormatVersion: '2010-09-09'
Transform: AWS::Serverless-2016-10-31
Description: GPU and CPU Image Processing Benchmark with AWS Batch using EC2

Parameters:
  MaxvCpus:
    Type: Number
    Default: 64
    Description: Maximum vCPUs for the compute environment
    
  GPUInstanceTypes:
    Type: CommaDelimitedList
    Default: g4dn.4xlarge,g4dn.8xlarge,g5.4xlarge
    Description: List of GPU instance types with at least 64GB memory

  CPUInstanceTypes:
    Type: CommaDelimitedList
    Default: r5.4xlarge,r5.8xlarge,m5.8xlarge,c5.18xlarge
    Description: List of CPU instance types with at least 64GB memory

Resources:
  # IAM Roles
  BatchServiceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: batch.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSBatchServiceRole'

  BatchInstanceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: ec2.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role'
        - 'arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess'

  BatchInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles:
        - !Ref BatchInstanceRole

  JobRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: ecs-tasks.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess'

  # Launch Template with User Data to configure instance
  GpuLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    Properties:
      LaunchTemplateData:
        UserData:
          Fn::Base64: |
            Content-Type: multipart/mixed; boundary="==BOUNDARY=="
            MIME-Version: 1.0

            --==BOUNDARY==
            Content-Type: text/x-shellscript; charset="us-ascii"

            #!/bin/bash
            # Set up logging
            exec > >(tee /var/log/user-data.log|logger -t user-data -s 2>/dev/console) 2>&1
            echo "Starting GPU configuration for AWS Batch: $(date)"

            # Configure ECS - critical for joining the cluster
            echo "ECS_CLUSTER=${AWS::StackName}" >> /etc/ecs/ecs.config
            echo "ECS_ENABLE_GPU_SUPPORT=true" >> /etc/ecs/ecs.config
            echo "ECS_ENABLE_TASK_IAM_ROLE=true" >> /etc/ecs/ecs.config
            echo "ECS_ENABLE_TASK_ENI=true" >> /etc/ecs/ecs.config
            echo "ECS_UPDATES_ENABLED=true" >> /etc/ecs/ecs.config
            echo "ECS_ENABLE_CONTAINER_METADATA=true" >> /etc/ecs/ecs.config
            
            # Restart ECS agent to apply changes
            systemctl restart ecs

            # Print out important info for debugging
            echo "System information summary:"
            echo "=========================="
            echo "Date: $(date)"
            echo "Hostname: $(hostname)"
            echo "ECS Agent status: $(systemctl is-active ecs)"
            echo "Docker status: $(systemctl is-active docker)"
            echo "NVIDIA drivers loaded: $(lsmod | grep -c nvidia || echo "No")"
            echo "=========================="

            echo "GPU Configuration complete: $(date)"

            --==BOUNDARY==--

  # Create VPC with public subnets for EC2 instances
  BatchVPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: 10.0.0.0/16
      EnableDnsSupport: true
      EnableDnsHostnames: true
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-VPC"

  InternetGateway:
    Type: AWS::EC2::InternetGateway
    Properties:
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-IGW"

  VPCGatewayAttachment:
    Type: AWS::EC2::VPCGatewayAttachment
    Properties:
      VpcId: !Ref BatchVPC
      InternetGatewayId: !Ref InternetGateway

  PublicSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref BatchVPC
      CidrBlock: 10.0.0.0/24
      MapPublicIpOnLaunch: true
      AvailabilityZone: !Select [0, !GetAZs ""]
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-PublicSubnet1"

  PublicSubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref BatchVPC
      CidrBlock: 10.0.1.0/24
      MapPublicIpOnLaunch: true
      AvailabilityZone: !Select [1, !GetAZs ""]
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-PublicSubnet2"

  PublicRouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref BatchVPC
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-PublicRouteTable"

  PublicRoute:
    Type: AWS::EC2::Route
    DependsOn: VPCGatewayAttachment
    Properties:
      RouteTableId: !Ref PublicRouteTable
      DestinationCidrBlock: 0.0.0.0/0
      GatewayId: !Ref InternetGateway

  PublicSubnet1RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnet1
      RouteTableId: !Ref PublicRouteTable

  PublicSubnet2RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnet2
      RouteTableId: !Ref PublicRouteTable

  BatchSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security group for AWS Batch compute environment
      VpcId: !Ref BatchVPC
      SecurityGroupEgress:
        - IpProtocol: -1
          CidrIp: 0.0.0.0/0
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-BatchSG"

  # Batch Compute Environments
  GpuComputeEnvironment:
    Type: AWS::Batch::ComputeEnvironment
    DependsOn: VPCGatewayAttachment
    Properties:
      Type: MANAGED
      ServiceRole: !GetAtt BatchServiceRole.Arn
      ComputeResources:
        Type: EC2
        MaxvCpus: !Ref MaxvCpus
        MinvCpus: 0
        DesiredvCpus: 0
        InstanceTypes: !Ref GPUInstanceTypes
        Subnets: 
          - !Ref PublicSubnet1
          - !Ref PublicSubnet2
        SecurityGroupIds:
          - !Ref BatchSecurityGroup
        InstanceRole: !GetAtt BatchInstanceProfile.Arn
        LaunchTemplate:
          LaunchTemplateId: !Ref GpuLaunchTemplate
          Version: $Latest
        AllocationStrategy: BEST_FIT_PROGRESSIVE
        Ec2Configuration:
          - ImageType: ECS_AL2023_NVIDIA
            ImageIdOverride: ami-054c79d378ed2f549
      State: ENABLED

  CPUComputeEnvironment:
    Type: AWS::Batch::ComputeEnvironment
    DependsOn: VPCGatewayAttachment
    Properties:
      Type: MANAGED
      ServiceRole: !GetAtt BatchServiceRole.Arn
      ComputeResources:
        Type: EC2
        MaxvCpus: !Ref MaxvCpus
        MinvCpus: 0
        DesiredvCpus: 0
        InstanceTypes: !Ref CPUInstanceTypes
        Subnets: 
          - !Ref PublicSubnet1
          - !Ref PublicSubnet2
        SecurityGroupIds:
          - !Ref BatchSecurityGroup
        InstanceRole: !GetAtt BatchInstanceProfile.Arn
        AllocationStrategy: BEST_FIT_PROGRESSIVE
      State: ENABLED

  # Job Queues
  GpuJobQueue:
    Type: AWS::Batch::JobQueue
    Properties:
      ComputeEnvironmentOrder:
        - ComputeEnvironment: !Ref GpuComputeEnvironment
          Order: 1
      Priority: 1
      State: ENABLED

  CPUJobQueue:
    Type: AWS::Batch::JobQueue
    Properties:
      ComputeEnvironmentOrder:
        - ComputeEnvironment: !Ref CPUComputeEnvironment
          Order: 1
      Priority: 1
      State: ENABLED

  # Job Definitions
  ImageProcessingJobDefinition:
    Type: AWS::Batch::JobDefinition
    Properties:
      Type: container
      JobDefinitionName: gpu-image-processing-benchmark
      Timeout:
        AttemptDurationSeconds: 14400  # 4 hours
      ContainerProperties:
        Image: nvidia/cuda:12.0.1-cudnn8-runtime-ubuntu22.04
        Command:
          - bash
          - -c
          - |
            echo "Starting GPU Benchmark"
            nvidia-smi
            
            # Install Python and necessary libraries
            apt-get update && apt-get install -y python3-pip libgl1-mesa-glx libglib2.0-0
            pip3 install numpy pillow opencv-python torch torchvision
            
            # Create benchmark script
            cat << 'EOF' > /tmp/benchmark.py
            import time
            import numpy as np
            import torch
            import cv2
            import os
            import gc
            
            def benchmark_gpu():
                print("=== GPU Benchmark for Image Processing ===")
                print("CUDA available:", torch.cuda.is_available())
                if torch.cuda.is_available():
                    print(f"GPU Device: {torch.cuda.get_device_name(0)}")
                    print(f"CUDA Version: {torch.version.cuda}")
                    print(f"CUDA Capability: {torch.cuda.get_device_capability(0)}")
                    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
                    
                    # 1. Matrix Operations Benchmark
                    print("\n=== 1. Running Matrix Operations Benchmark ===")
                    sizes = [1000, 2000, 5000]
                    for size in sizes:
                        # Clear GPU memory before each test
                        torch.cuda.empty_cache()
                        gc.collect()
                        
                        start = time.time()
                        
                        # Create large tensors on GPU
                        a = torch.randn(size, size, device='cuda')
                        b = torch.randn(size, size, device='cuda')
                        
                        # Matrix multiplication
                        c = torch.matmul(a, b)
                        torch.cuda.synchronize()  # Wait for operations to complete
                        
                        end = time.time()
                        print(f"Matrix multiplication {size}x{size}: {end - start:.2f} seconds")
                        
                        # Free memory
                        del a, b, c
                        torch.cuda.empty_cache()
                        gc.collect()
                    
                    # 2. Image Processing Benchmark
                    print("\n=== 2. Running Image Processing Benchmark ===")
                    img_sizes = [2048, 4096]
                    batch_sizes = [1, 2, 4]
                    
                    for img_size in img_sizes:
                        print(f"\nProcessing images of size {img_size}x{img_size}")
                        # Generate synthetic image
                        img = np.random.randint(0, 256, (img_size, img_size, 3), dtype=np.uint8)
                        
                        for batch_size in batch_sizes:
                            # Clear GPU memory before each test
                            torch.cuda.empty_cache()
                            gc.collect()
                            
                            # Convert to torch tensor on GPU
                            start = time.time()
                            img_tensor = torch.from_numpy(img).permute(2, 0, 1).float().cuda() / 255.0
                            
                            # Create batch
                            batch = img_tensor.unsqueeze(0).repeat(batch_size, 1, 1, 1)
                            
                            # Image processing operations
                            
                            # Convolutions
                            conv1 = torch.nn.Conv2d(3, 16, kernel_size=3, padding=1).cuda()
                            conv2 = torch.nn.Conv2d(16, 32, kernel_size=3, padding=1).cuda()
                            out = conv1(batch)
                            out = torch.nn.functional.relu(out)
                            out = conv2(out)
                            out = torch.nn.functional.relu(out)
                            
                            # Pooling
                            out = torch.nn.functional.max_pool2d(out, 2)
                            
                            # More convolutions
                            conv3 = torch.nn.Conv2d(32, 64, kernel_size=3, padding=1).cuda()
                            out = conv3(out)
                            out = torch.nn.functional.relu(out)
                            
                            # Batch normalization
                            bn = torch.nn.BatchNorm2d(64).cuda()
                            out = bn(out)
                            
                            # Final pooling
                            out = torch.nn.functional.max_pool2d(out, 2)
                            
                            torch.cuda.synchronize()  # Wait for operations to complete
                            
                            end = time.time()
                            print(f"Batch size {batch_size}: {end - start:.2f} seconds")
                            
                            # Free memory
                            del img_tensor, batch, out, conv1, conv2, conv3, bn
                            torch.cuda.empty_cache()
                            gc.collect()
                    
                    # 3. Advanced Image Transformations
                    print("\n=== 3. Running Advanced Image Transformations ===")
                    img_size = 2048
                    img = np.random.randint(0, 256, (img_size, img_size, 3), dtype=np.uint8)
                    
                    # Clear GPU memory before test
                    torch.cuda.empty_cache()
                    gc.collect()
                    
                    img_tensor = torch.from_numpy(img).permute(2, 0, 1).float().cuda() / 255.0
                    
                    # Record execution times for various operations
                    operations = {}
                    
                    # Gaussian Blur
                    start = time.time()
                    kernel_size = 15
                    sigma = 5.0
                    channels = img_tensor.shape[0]
                    
                    # Create 2D Gaussian kernel
                    kernel_size = kernel_size - kernel_size % 2  # Ensure odd size
                    kernel_x = torch.arange(kernel_size, device='cuda') - kernel_size // 2
                    kernel = torch.exp(-(kernel_x.view(-1, 1) ** 2 + kernel_x.view(1, -1) ** 2) / (2 * sigma ** 2))
                    kernel = kernel / kernel.sum()
                    
                    # Apply convolution for each channel
                    kernel = kernel.view(1, 1, kernel_size, kernel_size)
                    kernel = kernel.repeat(channels, 1, 1, 1)
                    padded = torch.nn.functional.pad(img_tensor.unsqueeze(0), (kernel_size//2, kernel_size//2, kernel_size//2, kernel_size//2), mode='reflect')
                    blurred = torch.nn.functional.conv2d(padded, kernel, groups=channels)
                    torch.cuda.synchronize()
                    operations["Gaussian Blur"] = time.time() - start
                    
                    # Edge Detection (Sobel)
                    start = time.time()
                    sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32, device='cuda').view(1, 1, 3, 3)
                    sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32, device='cuda').view(1, 1, 3, 3)
                    
                    padded = torch.nn.functional.pad(blurred, (1, 1, 1, 1), mode='reflect')
                    g_x = torch.nn.functional.conv2d(padded, sobel_x.repeat(channels, 1, 1, 1), groups=channels)
                    g_y = torch.nn.functional.conv2d(padded, sobel_y.repeat(channels, 1, 1, 1), groups=channels)
                    edges = torch.sqrt(g_x**2 + g_y**2)
                    torch.cuda.synchronize()
                    operations["Edge Detection"] = time.time() - start
                    
                    # Print results
                    print("\nAdvanced Operations Timing:")
                    for op, time_taken in operations.items():
                        print(f"{op}: {time_taken:.2f} seconds")
                    
                    # Free memory
                    del img_tensor, kernel, padded, blurred, sobel_x, sobel_y, g_x, g_y, edges
                    torch.cuda.empty_cache()
                    
                    print("\n=== GPU Benchmark Complete ===")
                else:
                    print("CUDA is not available. Cannot run GPU benchmark.")
            
            if __name__ == "__main__":
                benchmark_gpu()
            EOF
            
            # Run the benchmark
            python3 /tmp/benchmark.py
        Vcpus: 16             # Set for g4dn.4xlarge (16 vCPUs)
        Memory: 61440         # 60GB (g4dn.4xlarge has 64GB total)
        ResourceRequirements:
          - Type: GPU
            Value: "1"
        JobRoleArn: !GetAtt JobRole.Arn
        LinuxParameters:
          Devices:
            - HostPath: /dev/nvidia0
              ContainerPath: /dev/nvidia0
              Permissions:
                - READ
                - WRITE

  CPUImageProcessingJobDefinition:
    Type: AWS::Batch::JobDefinition
    Properties:
      Type: container
      JobDefinitionName: cpu-image-processing-benchmark
      Timeout:
        AttemptDurationSeconds: 43200  # 12 hours - CPUs may need more time
      ContainerProperties:
        Image: nvidia/cuda:12.0.1-cudnn8-runtime-ubuntu22.04  # Using the same image as GPU for consistency
        Command:
          - bash
          - -c
          - |
            echo "Starting CPU Benchmark"
            
            # Install Python and necessary libraries
            apt-get update && apt-get install -y python3-pip libgl1-mesa-glx libglib2.0-0
            pip3 install numpy pillow opencv-python torch torchvision
            
            # Create benchmark script
            cat << 'EOF' > /tmp/benchmark.py
            import time
            import numpy as np
            import torch
            import cv2
            import os
            import gc
            
            def benchmark_cpu():
                print("=== CPU Benchmark for Image Processing ===")
                
                # Force CPU computations
                torch.set_num_threads(os.cpu_count())
                device = torch.device('cpu')
                print(f"Using CPU with {os.cpu_count()} threads")
                print(f"PyTorch version: {torch.__version__}")
                
                # 1. Matrix Operations Benchmark
                print("\n=== 1. Running Matrix Operations Benchmark ===")
                sizes = [1000, 2000, 5000]  # Same sizes as GPU benchmark
                for size in sizes:
                    # Clear memory before each test
                    gc.collect()
                    
                    start = time.time()
                    
                    # Create large tensors on CPU
                    a = torch.randn(size, size, device=device)
                    b = torch.randn(size, size, device=device)
                    
                    # Matrix multiplication
                    c = torch.matmul(a, b)
                    
                    end = time.time()
                    print(f"Matrix multiplication {size}x{size}: {end - start:.2f} seconds")
                    
                    # Free memory
                    del a, b, c
                    gc.collect()
                
                # 2. Image Processing Benchmark
                print("\n=== 2. Running Image Processing Benchmark ===")
                # Now we can use the same sizes as GPU since we have more memory
                img_sizes = [2048, 4096]  # Same as GPU benchmark
                batch_sizes = [1, 2, 4]   # Same as GPU benchmark
                
                for img_size in img_sizes:
                    print(f"\nProcessing images of size {img_size}x{img_size}")
                    # Generate synthetic image
                    img = np.random.randint(0, 256, (img_size, img_size, 3), dtype=np.uint8)
                    
                    for batch_size in batch_sizes:
                        # Clear memory before each test
                        gc.collect()
                        
                        try:
                            # Convert to torch tensor on CPU
                            start = time.time()
                            img_tensor = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0
                            
                            # Create batch
                            batch = img_tensor.unsqueeze(0).repeat(batch_size, 1, 1, 1)
                            
                            # Image processing operations
                            
                            # Convolutions
                            conv1 = torch.nn.Conv2d(3, 16, kernel_size=3, padding=1)
                            conv2 = torch.nn.Conv2d(16, 32, kernel_size=3, padding=1)
                            out = conv1(batch)
                            out = torch.nn.functional.relu(out)
                            out = conv2(out)
                            out = torch.nn.functional.relu(out)
                            
                            # Pooling
                            out = torch.nn.functional.max_pool2d(out, 2)
                            
                            # More convolutions
                            conv3 = torch.nn.Conv2d(32, 64, kernel_size=3, padding=1)
                            out = conv3(out)
                            out = torch.nn.functional.relu(out)
                            
                            # Batch normalization
                            bn = torch.nn.BatchNorm2d(64)
                            out = bn(out)
                            
                            # Final pooling
                            out = torch.nn.functional.max_pool2d(out, 2)
                            
                            end = time.time()
                            print(f"Batch size {batch_size}: {end - start:.2f} seconds")
                            
                            # Free memory
                            del img_tensor, batch, out, conv1, conv2, conv3, bn
                            gc.collect()
                        except RuntimeError as e:
                            if "out of memory" in str(e):
                                print(f"Not enough memory for image size {img_size} with batch size {batch_size}")
                            else:
                                print(f"Error processing image size {img_size} with batch size {batch_size}: {e}")
                
                # 3. Advanced Image Transformations
                print("\n=== 3. Running Advanced Image Transformations ===")
                img_size = 2048  # Same size as GPU benchmark
                img = np.random.randint(0, 256, (img_size, img_size, 3), dtype=np.uint8)
                img_tensor = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0
                
                # Record execution times for various operations
                operations = {}
                
                # Gaussian Blur
                start = time.time()
                kernel_size = 15
                sigma = 5.0
                channels = img_tensor.shape[0]
                
                # Create 2D Gaussian kernel
                kernel_size = kernel_size - kernel_size % 2  # Ensure odd size
                kernel_x = torch.arange(kernel_size) - kernel_size // 2
                kernel = torch.exp(-(kernel_x.view(-1, 1) ** 2 + kernel_x.view(1, -1) ** 2) / (2 * sigma ** 2))
                kernel = kernel / kernel.sum()
                
                # Apply convolution for each channel
                kernel = kernel.view(1, 1, kernel_size, kernel_size)
                kernel = kernel.repeat(channels, 1, 1, 1)
                padded = torch.nn.functional.pad(img_tensor.unsqueeze(0), (kernel_size//2, kernel_size//2, kernel_size//2, kernel_size//2), mode='reflect')
                blurred = torch.nn.functional.conv2d(padded, kernel, groups=channels)
                
                operations["Gaussian Blur"] = time.time() - start
                
                # Edge Detection (Sobel)
                start = time.time()
                sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3)
                sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3)
                
                padded = torch.nn.functional.pad(blurred, (1, 1, 1, 1), mode='reflect')
                g_x = torch.nn.functional.conv2d(padded, sobel_x.repeat(channels, 1, 1, 1), groups=channels)
                g_y = torch.nn.functional.conv2d(padded, sobel_y.repeat(channels, 1, 1, 1), groups=channels)
                edges = torch.sqrt(g_x**2 + g_y**2)
                
                operations["Edge Detection"] = time.time() - start
                
                # Print results
                print("\nAdvanced Operations Timing:")
                for op, time_taken in operations.items():
                    print(f"{op}: {time_taken:.2f} seconds")
                
                print("\n=== CPU Benchmark Complete ===")
            
            if __name__ == "__main__":
                benchmark_cpu()
            EOF
            
            # Run the benchmark
            python3 /tmp/benchmark.py
        Vcpus: 16             # Using 16 vCPUs
        Memory: 61440         # 60GB of RAM
        JobRoleArn: !GetAtt JobRole.Arn

  # CloudWatch Logs Group
  BatchLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: /aws/batch/gpu-benchmark
      RetentionInDays: 14

Outputs:
  GpuJobQueue:
    Description: ARN of the GPU Job Queue
    Value: !Ref GpuJobQueue
    
  CpuJobQueue:
    Description: ARN of the CPU Job Queue
    Value: !Ref CPUJobQueue
    
  GpuJobDefinition:
    Description: ARN of the GPU Job Definition
    Value: !Ref ImageProcessingJobDefinition

  CpuJobDefinition:
    Description: ARN of the CPU Job Definition
    Value: !Ref CPUImageProcessingJobDefinition
    
  SubmitGpuJobCommand:
    Description: Command to submit a GPU job
    Value: !Sub "aws batch submit-job --job-name gpu-benchmark-$(date +%Y%m%d%H%M%S) --job-queue ${GpuJobQueue} --job-definition ${ImageProcessingJobDefinition}"

  SubmitCpuJobCommand:
    Description: Command to submit a CPU job
    Value: !Sub "aws batch submit-job --job-name cpu-benchmark-$(date +%Y%m%d%H%M%S) --job-queue ${CPUJobQueue} --job-definition ${CPUImageProcessingJobDefinition}"

  ListRunningGpuJobsCommand:
    Description: Command to list running GPU jobs
    Value: !Sub "aws batch list-jobs --job-queue ${GpuJobQueue} --job-status RUNNING"

  ListRunningCpuJobsCommand:
    Description: Command to list running CPU jobs
    Value: !Sub "aws batch list-jobs --job-queue ${CPUJobQueue} --job-status RUNNING"

  DescribeJobsCommand:
    Description: Command to describe specific jobs (replace job-id-1 with actual job IDs)
    Value: "aws batch describe-jobs --jobs job-id-1 job-id-2"

  GetGpuJobLogsCommand:
    Description: Command to get logs for a GPU job (replace job-id with actual ID)
    Value: !Sub "LOG_STREAM=$(aws batch describe-jobs --jobs job-id --query 'jobs[0].container.logStreamName' --output text) && aws logs get-log-events --log-group-name /aws/batch/gpu-benchmark --log-stream-name $LOG_STREAM"

  GetCpuJobLogsCommand:
    Description: Command to get logs for a CPU job (replace job-id with actual ID)
    Value: !Sub "LOG_STREAM=$(aws batch describe-jobs --jobs job-id --query 'jobs[0].container.logStreamName' --output text) && aws logs get-log-events --log-group-name /aws/batch/gpu-benchmark --log-stream-name $LOG_STREAM"

  GetBenchmarkResultsCommand:
    Description: Command to extract just the benchmark timing results (replace job-id with actual ID)
    Value: !Sub "LOG_STREAM=$(aws batch describe-jobs --jobs job-id --query 'jobs[0].container.logStreamName' --output text) && aws logs get-log-events --log-group-name /aws/batch/gpu-benchmark --log-stream-name $LOG_STREAM --output text | grep -E 'multiplication|Batch size|Operation'"

  MonitorJobStatusCommand:
    Description: Command to monitor job status (replace job-id with actual ID)
    Value: "watch -n 10 'aws batch describe-jobs --jobs job-id --query \"jobs[0].{JobName:jobName,Status:status,Reason:statusReason,StartedAt:startedAt,StoppedAt:stoppedAt}\"'"

  VpcId:
    Description: ID of the VPC created for the Batch environment
    Value: !Ref BatchVPC
    
  SubnetIds:
    Description: IDs of the subnets created for the Batch environment
    Value: !Join [", ", [!Ref PublicSubnet1, !Ref PublicSubnet2]]
