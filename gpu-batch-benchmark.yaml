# gpu-batch-benchmark.yaml
AWSTemplateFormatVersion: '2010-09-09'
Transform: AWS::Serverless-2016-10-31
Description: GPU-optimized Image Processing Benchmark with AWS Batch using EC2

Parameters:
  MaxvCpus:
    Type: Number
    Default: 64
    Description: Maximum vCPUs for the compute environment
    
  InstanceTypes:
    Type: CommaDelimitedList
    Default: g4dn.xlarge,g5.xlarge,p3.2xlarge
    Description: List of GPU instance types to use for the compute environment

Resources:
  # IAM Roles
  BatchServiceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: batch.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSBatchServiceRole'

  BatchInstanceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: ec2.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role'
        - 'arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess'

  BatchInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles:
        - !Ref BatchInstanceRole

  JobRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: ecs-tasks.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess'

  # Launch Template with User Data to configure instance
  GpuLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    Properties:
      LaunchTemplateData:
        UserData:
          Fn::Base64: !Sub |
            #!/bin/bash
            # Change to ubuntu user for driver installation as per AMI requirements
            sudo -u ubuntu bash -c '
              # Update and install dependencies
              sudo apt-get update -y
              sudo apt-get install -y build-essential
              
              # Verify NVIDIA drivers are properly installed
              nvidia-smi
              
              # Setup Docker to use NVIDIA runtime
              sudo tee /etc/docker/daemon.json <<EOF
            {
                "default-runtime": "nvidia",
                "runtimes": {
                    "nvidia": {
                        "path": "/usr/bin/nvidia-container-runtime",
                        "runtimeArgs": []
                    }
                }
            }
            EOF
              
              # Restart docker service to apply changes
              sudo systemctl restart docker
            '

  # Create VPC with public subnets for EC2 instances
  BatchVPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: 10.0.0.0/16
      EnableDnsSupport: true
      EnableDnsHostnames: true
      Tags:
        - Key: Name
          Value: !Sub "\${AWS::StackName}-VPC"

  InternetGateway:
    Type: AWS::EC2::InternetGateway
    Properties:
      Tags:
        - Key: Name
          Value: !Sub "\${AWS::StackName}-IGW"

  VPCGatewayAttachment:
    Type: AWS::EC2::VPCGatewayAttachment
    Properties:
      VpcId: !Ref BatchVPC
      InternetGatewayId: !Ref InternetGateway

  PublicSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref BatchVPC
      CidrBlock: 10.0.0.0/24
      MapPublicIpOnLaunch: true
      AvailabilityZone: !Select [0, !GetAZs ""]
      Tags:
        - Key: Name
          Value: !Sub "\${AWS::StackName}-PublicSubnet1"

  PublicSubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref BatchVPC
      CidrBlock: 10.0.1.0/24
      MapPublicIpOnLaunch: true
      AvailabilityZone: !Select [1, !GetAZs ""]
      Tags:
        - Key: Name
          Value: !Sub "\${AWS::StackName}-PublicSubnet2"

  PublicRouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref BatchVPC
      Tags:
        - Key: Name
          Value: !Sub "\${AWS::StackName}-PublicRouteTable"

  PublicRoute:
    Type: AWS::EC2::Route
    DependsOn: VPCGatewayAttachment
    Properties:
      RouteTableId: !Ref PublicRouteTable
      DestinationCidrBlock: 0.0.0.0/0
      GatewayId: !Ref InternetGateway

  PublicSubnet1RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnet1
      RouteTableId: !Ref PublicRouteTable

  PublicSubnet2RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnet2
      RouteTableId: !Ref PublicRouteTable

  BatchSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security group for AWS Batch compute environment
      VpcId: !Ref BatchVPC
      SecurityGroupEgress:
        - IpProtocol: -1
          CidrIp: 0.0.0.0/0
      Tags:
        - Key: Name
          Value: !Sub "\${AWS::StackName}-BatchSG"

  # Batch Compute Environment
  GpuComputeEnvironment:
    Type: AWS::Batch::ComputeEnvironment
    DependsOn: VPCGatewayAttachment
    Properties:
      Type: MANAGED
      ServiceRole: !GetAtt BatchServiceRole.Arn
      ComputeResources:
        Type: EC2
        MaxvCpus: !Ref MaxvCpus
        MinvCpus: 0
        DesiredvCpus: 0
        InstanceTypes: !Ref InstanceTypes
        Subnets: 
          - !Ref PublicSubnet1
          - !Ref PublicSubnet2
        SecurityGroupIds:
          - !Ref BatchSecurityGroup
        InstanceRole: !GetAtt BatchInstanceProfile.Arn
        LaunchTemplate:
          LaunchTemplateId: !Ref GpuLaunchTemplate
          Version: \$Latest
        AllocationStrategy: BEST_FIT_PROGRESSIVE
        EC2Configuration:
          - ImageType: ECS_AL2_NVIDIA
            ImageIdOverride: ami-072324a133dc4f9c9  # Latest NVIDIA GPU-Optimized AMI (Deep Learning Base Ubuntu 20.04)
      State: ENABLED

  # Job Queue
  GpuJobQueue:
    Type: AWS::Batch::JobQueue
    Properties:
      ComputeEnvironmentOrder:
        - ComputeEnvironment: !Ref GpuComputeEnvironment
          Order: 1
      Priority: 1
      State: ENABLED

  # Job Definition
  ImageProcessingJobDefinition:
    Type: AWS::Batch::JobDefinition
    Properties:
      Type: container
      JobDefinitionName: gpu-image-processing-benchmark
      ContainerProperties:
        Image: nvidia/cuda:12.0.1-cudnn8-runtime-ubuntu22.04
        Command:
          - bash
          - -c
          - |
            echo "Starting GPU Benchmark"
            nvidia-smi
            
            # Install Python and necessary libraries
            apt-get update && apt-get install -y python3-pip
            pip3 install numpy pillow opencv-python torch torchvision
            
            # Create benchmark script
            cat << 'EOF' > /tmp/benchmark.py
            import time
            import numpy as np
            import torch
            import cv2
            import os
            
            def benchmark_gpu():
                print("=== GPU Benchmark for Image Processing ===")
                print("CUDA available:", torch.cuda.is_available())
                if torch.cuda.is_available():
                    print(f"GPU Device: {torch.cuda.get_device_name(0)}")
                    print(f"CUDA Version: {torch.version.cuda}")
                    print(f"CUDA Capability: {torch.cuda.get_device_capability(0)}")
                    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
                    
                    # 1. Matrix Operations Benchmark
                    print("\n=== 1. Running Matrix Operations Benchmark ===")
                    sizes = [5000, 10000, 15000]
                    for size in sizes:
                        start = time.time()
                        
                        # Create large tensors on GPU
                        a = torch.randn(size, size, device='cuda')
                        b = torch.randn(size, size, device='cuda')
                        
                        # Matrix multiplication
                        c = torch.matmul(a, b)
                        torch.cuda.synchronize()  # Wait for operations to complete
                        
                        end = time.time()
                        print(f"Matrix multiplication {size}x{size}: {end - start:.2f} seconds")
                    
                    # 2. Image Processing Benchmark
                    print("\n=== 2. Running Image Processing Benchmark ===")
                    img_sizes = [4096, 8192]
                    batch_sizes = [1, 4, 8]
                    
                    for img_size in img_sizes:
                        print(f"\nProcessing images of size {img_size}x{img_size}")
                        # Generate synthetic image
                        img = np.random.randint(0, 256, (img_size, img_size, 3), dtype=np.uint8)
                        
                        for batch_size in batch_sizes:
                            # Convert to torch tensor on GPU
                            start = time.time()
                            img_tensor = torch.from_numpy(img).permute(2, 0, 1).float().cuda() / 255.0
                            
                            # Create batch
                            batch = img_tensor.unsqueeze(0).repeat(batch_size, 1, 1, 1)
                            
                            # Image processing operations
                            
                            # Convolutions
                            conv1 = torch.nn.Conv2d(3, 16, kernel_size=3, padding=1).cuda()
                            conv2 = torch.nn.Conv2d(16, 32, kernel_size=3, padding=1).cuda()
                            out = conv1(batch)
                            out = torch.nn.functional.relu(out)
                            out = conv2(out)
                            out = torch.nn.functional.relu(out)
                            
                            # Pooling
                            out = torch.nn.functional.max_pool2d(out, 2)
                            
                            # More convolutions
                            conv3 = torch.nn.Conv2d(32, 64, kernel_size=3, padding=1).cuda()
                            out = conv3(out)
                            out = torch.nn.functional.relu(out)
                            
                            # Batch normalization
                            bn = torch.nn.BatchNorm2d(64).cuda()
                            out = bn(out)
                            
                            # Final pooling
                            out = torch.nn.functional.max_pool2d(out, 2)
                            
                            torch.cuda.synchronize()  # Wait for operations to complete
                            
                            end = time.time()
                            print(f"Batch size {batch_size}: {end - start:.2f} seconds")
                    
                    # 3. Advanced Image Transformations
                    print("\n=== 3. Running Advanced Image Transformations ===")
                    img_size = 4096
                    img = np.random.randint(0, 256, (img_size, img_size, 3), dtype=np.uint8)
                    img_tensor = torch.from_numpy(img).permute(2, 0, 1).float().cuda() / 255.0
                    
                    # Record execution times for various operations
                    operations = {}
                    
                    # Gaussian Blur
                    start = time.time()
                    kernel_size = 15
                    sigma = 5.0
                    channels = img_tensor.shape[0]
                    
                    # Create 2D Gaussian kernel
                    kernel_size = kernel_size - kernel_size % 2  # Ensure odd size
                    kernel_x = torch.arange(kernel_size, device='cuda') - kernel_size // 2
                    kernel = torch.exp(-(kernel_x.view(-1, 1) ** 2 + kernel_x.view(1, -1) ** 2) / (2 * sigma ** 2))
                    kernel = kernel / kernel.sum()
                    
                    # Apply convolution for each channel
                    kernel = kernel.view(1, 1, kernel_size, kernel_size)
                    kernel = kernel.repeat(channels, 1, 1, 1)
                    padded = torch.nn.functional.pad(img_tensor.unsqueeze(0), (kernel_size//2, kernel_size//2, kernel_size//2, kernel_size//2), mode='reflect')
                    blurred = torch.nn.functional.conv2d(padded, kernel, groups=channels)
                    torch.cuda.synchronize()
                    operations["Gaussian Blur"] = time.time() - start
                    
                    # Edge Detection (Sobel)
                    start = time.time()
                    sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32, device='cuda').view(1, 1, 3, 3)
                    sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32, device='cuda').view(1, 1, 3, 3)
                    
                    padded = torch.nn.functional.pad(blurred, (1, 1, 1, 1), mode='reflect')
                    g_x = torch.nn.functional.conv2d(padded, sobel_x.repeat(channels, 1, 1, 1), groups=channels)
                    g_y = torch.nn.functional.conv2d(padded, sobel_y.repeat(channels, 1, 1, 1), groups=channels)
                    edges = torch.sqrt(g_x**2 + g_y**2)
                    torch.cuda.synchronize()
                    operations["Edge Detection"] = time.time() - start
                    
                    # Print results
                    print("\nAdvanced Operations Timing:")
                    for op, time_taken in operations.items():
                        print(f"{op}: {time_taken:.2f} seconds")
                    
                    print("\n=== GPU Benchmark Complete ===")
                else:
                    print("CUDA is not available. Cannot run GPU benchmark.")
            
            if __name__ == "__main__":
                benchmark_gpu()
            EOF
            
            # Run the benchmark
            python3 /tmp/benchmark.py
        Vcpus: 8
        Memory: 16384
        ResourceRequirements:
          - Type: GPU
            Value: "1"
        JobRoleArn: !GetAtt JobRole.Arn
        LinuxParameters:
          Devices:
            - HostPath: /dev/nvidia0
              ContainerPath: /dev/nvidia0
              Permissions:
                - READ
                - WRITE

  # CloudWatch Logs Group
  BatchLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: /aws/batch/gpu-benchmark
      RetentionInDays: 14

Outputs:
  JobQueue:
    Description: ARN of the Job Queue
    Value: !Ref GpuJobQueue
    
  JobDefinition:
    Description: ARN of the Job Definition
    Value: !Ref ImageProcessingJobDefinition
    
  SubmitJobCommand:
    Description: Command to submit a job
    Value: !Sub "aws batch submit-job --job-name gpu-benchmark-\$(date +%Y%m%d%H%M%S) --job-queue \${GpuJobQueue} --job-definition \${ImageProcessingJobDefinition}"
    
  VpcId:
    Description: ID of the VPC created for the Batch environment
    Value: !Ref BatchVPC
    
  SubnetIds:
    Description: IDs of the subnets created for the Batch environment
    Value: !Join [", ", [!Ref PublicSubnet1, !Ref PublicSubnet2]]