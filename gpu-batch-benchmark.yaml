# gpu-batch-benchmark.yaml
AWSTemplateFormatVersion: '2010-09-09'
Transform: AWS::Serverless-2016-10-31
Description: GPU and CPU Image Processing Benchmark with AWS Batch using EC2

Parameters:
  MaxvCpus:
    Type: Number
    Default: 64
    Description: Maximum vCPUs for the compute environment
    
  GPUInstanceTypes:
    Type: CommaDelimitedList
    Default: g5.8xlarge,g5.12xlarge,p3.2xlarge
    Description: List of GPU instance types with larger GPU memory (24GB A10G or 16GB V100)

  CPUInstanceTypes:
    Type: CommaDelimitedList
    Default: r5.4xlarge,r5.8xlarge,m5.8xlarge,c5.18xlarge
    Description: List of CPU instance types with at least 64GB memory

Resources:
  # IAM Roles
  BatchServiceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: batch.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSBatchServiceRole'

  BatchInstanceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: ec2.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role'
        - 'arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess'
      Policies:
        - PolicyName: EC2TagsAndECSAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'ec2:DescribeTags'
                  - 'ec2:DescribeInstances'
                  - 'ecs:ListClusters'
                  - 'ecs:DescribeClusters'
                Resource: '*'

  BatchInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles:
        - !Ref BatchInstanceRole

  JobRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: ecs-tasks.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess'

  # Launch Template with User Data to configure instance
  GpuLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    Properties:
      LaunchTemplateData:
        UserData:
          Fn::Base64: |
            Content-Type: multipart/mixed; boundary="==BOUNDARY=="
            MIME-Version: 1.0

            --==BOUNDARY==
            Content-Type: text/cloud-config; charset="us-ascii"

            #cloud-config
            write_files:
              - path: /etc/ecs/ecs.config
                owner: root:root
                permissions: '0644'
                content: |
                  ECS_ENABLE_GPU_SUPPORT=true
                  ECS_AVAILABLE_LOGGING_DRIVERS=["json-file","awslogs"]
                  ECS_CONTAINER_INSTANCE_TAGS={"GPUEnabled": "true"}
                  ECS_ENGINE_TASK_CLEANUP_WAIT_DURATION=1h

            --==BOUNDARY==
            Content-Type: text/x-shellscript; charset="us-ascii"

            #!/bin/bash
            # Set up logging with timestamps
            exec > >(tee /var/log/user-data.log | logger -t user-data -s 2>/dev/console) 2>&1
            echo "Starting GPU configuration for AWS Batch: $(date)"

            # Function to log with timestamps
            log() {
              echo "$(date '+%Y-%m-%d %H:%M:%S') - $1"
            }

            # Wait for system to stabilize
            log "Waiting for system initialization..."
            sleep 10

            # Handle fabric manager issue
            log "Handling NVIDIA services setup..."
            log "Disabling NVIDIA fabric manager (not needed for T4 GPUs)..."
            systemctl stop nvidia-fabricmanager.service 2>/dev/null || true
            systemctl disable nvidia-fabricmanager.service 2>/dev/null || true
            
            # Ensure NVIDIA persistence daemon is running
            log "Starting NVIDIA persistence daemon..."
            systemctl start nvidia-persistenced 2>/dev/null || true
            systemctl status nvidia-persistenced --no-pager || log "WARNING: NVIDIA persistence daemon may not be running"
            
            # Check for NVIDIA drivers
            log "Validating NVIDIA drivers..."
            if ! nvidia-smi; then
              log "ERROR: NVIDIA drivers not loaded properly!"
              # Try to reload NVIDIA drivers
              modprobe -r nvidia_uvm nvidia_drm nvidia_modeset nvidia
              modprobe nvidia
              modprobe nvidia_modeset
              modprobe nvidia_drm
              modprobe nvidia_uvm
              
              # Check again
              if ! nvidia-smi; then
                log "CRITICAL: Failed to load NVIDIA drivers after retry"
              else
                log "NVIDIA drivers loaded successfully after retry"
              fi
            fi

            # Verify NVIDIA container runtime installation
            log "Checking NVIDIA container runtime..."
            if [ -f /usr/bin/nvidia-container-runtime ]; then
              log "NVIDIA container runtime is installed at $(which nvidia-container-runtime)"
            else
              log "ERROR: NVIDIA container runtime is missing, attempting to install..."
              # For Amazon Linux 2
              amazon-linux-extras enable docker
              yum install -y nvidia-container-toolkit nvidia-container-runtime
              
              # Check if installation succeeded
              if [ -f /usr/bin/nvidia-container-runtime ]; then
                log "Successfully installed NVIDIA container runtime"
              else
                log "CRITICAL: Failed to install NVIDIA container runtime"
              fi
            fi

            # Configure Docker for NVIDIA runtime
            log "Configuring Docker for NVIDIA runtime..."
            mkdir -p /etc/docker
            cat > /etc/docker/daemon.json << 'EOF'
            {
              "default-runtime": "nvidia",
              "runtimes": {
                "nvidia": {
                  "path": "/usr/bin/nvidia-container-runtime",
                  "runtimeArgs": []
                }
              },
              "storage-driver": "overlay2"
            }
            EOF

            # Restart Docker to apply changes
            log "Restarting Docker service..."
            systemctl restart docker --no-block
            
            # Wait for Docker to fully initialize
            sleep 15
            
            # Verify Docker is running with NVIDIA runtime
            log "Verifying Docker service status..."
            systemctl status docker --no-pager
            
            # Test Docker configuration
            log "Docker runtime configuration:"
            docker info | grep -i runtime || log "WARNING: Failed to get Docker runtime info"
            
            # Test NVIDIA container capability
            log "Testing NVIDIA container capability..."
            docker run --rm --gpus all --log-driver=json-file nvidia/cuda:12.0.1-base-ubuntu22.04 nvidia-smi || log "WARNING: GPU container test failed"
            
            # Ensure ECS agent has proper GPU configuration
            log "Configuring ECS agent for GPU support..."
            
            # Backup original config
            if [ -f /etc/ecs/ecs.config ]; then
              cp /etc/ecs/ecs.config /etc/ecs/ecs.config.bak
              log "Backed up original ECS config"
            fi
            
            # Create a fresh ECS config with all required settings
            cat > /etc/ecs/ecs.config << 'EOF'
            ECS_ENABLE_GPU_SUPPORT=true
            ECS_AVAILABLE_LOGGING_DRIVERS=["json-file","awslogs"]
            ECS_CONTAINER_INSTANCE_TAGS={"GPUEnabled": "true"}
            ECS_ENGINE_TASK_CLEANUP_WAIT_DURATION=1h
            ECS_INSTANCE_ATTRIBUTES={"GPU": "true", "GPU_TYPE": "NVIDIA_T4"}
            ECS_ENABLE_TASK_IAM_ROLE=true
            ECS_ENABLE_TASK_ENI=true
            ECS_UPDATES_ENABLED=true
            ECS_ENABLE_CONTAINER_METADATA=true
            ECS_LOGLEVEL=debug
            # DO NOT set ECS_CLUSTER - let AWS Batch handle this automatically
            EOF
            
            log "Created new ECS config with GPU support"
            
            # Note: We intentionally do NOT start or configure the ECS agent here
            # The ECS agent will be started automatically by AWS Batch after this script completes
            # Starting it manually can cause a deadlock since cloud-init waits for this script to complete
            log "ECS agent will be started automatically after UserData script completes"
            
            # Display ECS config file contents
            log "ECS Config File Contents:"
            cat /etc/ecs/ecs.config
            
            # Check network connectivity to ECS endpoints
            log "Checking network connectivity to ECS endpoints..."
            ECS_ENDPOINTS=(
              "ecs.us-east-1.amazonaws.com"
              "ecs-a-1.us-east-1.amazonaws.com"
              "ecs-t-1.us-east-1.amazonaws.com"
              "ecs.us-west-2.amazonaws.com"
              "ecs-a-1.us-west-2.amazonaws.com"
              "ecs-t-1.us-west-2.amazonaws.com"
              "ecs.eu-west-1.amazonaws.com"
              "ecs-a-1.eu-west-1.amazonaws.com"
              "ecs-t-1.eu-west-1.amazonaws.com"
            )
            
            for endpoint in "${ECS_ENDPOINTS[@]}"; do
              if curl -s --connect-timeout 5 "https://$endpoint" > /dev/null; then
                log "Connection to $endpoint: SUCCESS"
              else
                log "Connection to $endpoint: FAILED"
              fi
            done
            
            # Print GPU status for verification
            log "GPU Status:"
            nvidia-smi
            
            # Final system information
            log "System information summary:"
            log "=========================="
            log "Date: $(date)"
            log "Hostname: $(hostname)"
            log "Kernel: $(uname -r)"
            log "ECS Agent status: $(systemctl is-active ecs)"
            log "Docker status: $(systemctl is-active docker)"
            log "NVIDIA drivers loaded: $(lsmod | grep -c nvidia || echo "No")"
            log "GPU detected: $(nvidia-smi -L || echo "No GPU detected")"
            log "=========================="
            
            log "GPU Configuration complete: $(date)"

            --==BOUNDARY==--

  # Create VPC with public subnets for EC2 instances
  BatchVPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: 10.0.0.0/16
      EnableDnsSupport: true
      EnableDnsHostnames: true
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-VPC"

  InternetGateway:
    Type: AWS::EC2::InternetGateway
    Properties:
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-IGW"

  VPCGatewayAttachment:
    Type: AWS::EC2::VPCGatewayAttachment
    Properties:
      VpcId: !Ref BatchVPC
      InternetGatewayId: !Ref InternetGateway

  PublicSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref BatchVPC
      CidrBlock: 10.0.0.0/24
      MapPublicIpOnLaunch: true
      AvailabilityZone: !Select [0, !GetAZs ""]
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-PublicSubnet1"

  PublicSubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref BatchVPC
      CidrBlock: 10.0.1.0/24
      MapPublicIpOnLaunch: true
      AvailabilityZone: !Select [1, !GetAZs ""]
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-PublicSubnet2"

  PublicRouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref BatchVPC
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-PublicRouteTable"

  PublicRoute:
    Type: AWS::EC2::Route
    DependsOn: VPCGatewayAttachment
    Properties:
      RouteTableId: !Ref PublicRouteTable
      DestinationCidrBlock: 0.0.0.0/0
      GatewayId: !Ref InternetGateway

  PublicSubnet1RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnet1
      RouteTableId: !Ref PublicRouteTable

  PublicSubnet2RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnet2
      RouteTableId: !Ref PublicRouteTable

  BatchSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security group for AWS Batch compute environment
      VpcId: !Ref BatchVPC
      SecurityGroupEgress:
        - IpProtocol: -1
          CidrIp: 0.0.0.0/0
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-BatchSG"

  # Batch Compute Environments
  GpuComputeEnvironment:
    Type: AWS::Batch::ComputeEnvironment
    DependsOn: VPCGatewayAttachment
    Properties:
      Type: MANAGED
      ServiceRole: !GetAtt BatchServiceRole.Arn
      ComputeResources:
        Type: EC2
        MaxvCpus: !Ref MaxvCpus
        MinvCpus: 0
        DesiredvCpus: 0
        InstanceTypes: !Ref GPUInstanceTypes
        Subnets: 
          - !Ref PublicSubnet1
          - !Ref PublicSubnet2
        SecurityGroupIds:
          - !Ref BatchSecurityGroup
        InstanceRole: !GetAtt BatchInstanceProfile.Arn
        LaunchTemplate:
          LaunchTemplateId: !Ref GpuLaunchTemplate
          Version: $Latest
        AllocationStrategy: BEST_FIT
        Ec2Configuration:
          - ImageType: ECS_AL2_NVIDIA
      State: ENABLED

  CPUComputeEnvironment: # By not including Ec2Configuration, we can use the default ECS_AL2
    Type: AWS::Batch::ComputeEnvironment
    DependsOn: VPCGatewayAttachment
    Properties:
      Type: MANAGED
      ServiceRole: !GetAtt BatchServiceRole.Arn
      ComputeResources:
        Type: EC2
        MaxvCpus: !Ref MaxvCpus
        MinvCpus: 0
        DesiredvCpus: 0
        InstanceTypes: !Ref CPUInstanceTypes
        Subnets: 
          - !Ref PublicSubnet1
          - !Ref PublicSubnet2
        SecurityGroupIds:
          - !Ref BatchSecurityGroup
        InstanceRole: !GetAtt BatchInstanceProfile.Arn
        AllocationStrategy: BEST_FIT_PROGRESSIVE
      State: ENABLED

  # Job Queues
  GpuJobQueue:
    Type: AWS::Batch::JobQueue
    Properties:
      ComputeEnvironmentOrder:
        - ComputeEnvironment: !Ref GpuComputeEnvironment
          Order: 1
      Priority: 1
      State: ENABLED

  CPUJobQueue:
    Type: AWS::Batch::JobQueue
    Properties:
      ComputeEnvironmentOrder:
        - ComputeEnvironment: !Ref CPUComputeEnvironment
          Order: 1
      Priority: 1
      State: ENABLED

  # Job Definitions
  ImageProcessingJobDefinition:
    Type: AWS::Batch::JobDefinition
    Properties:
      Type: container
      JobDefinitionName: gpu-image-processing-benchmark
      Timeout:
        AttemptDurationSeconds: 14400  # 4 hours
      ContainerProperties:
        Image: nvidia/cuda:12.0.1-cudnn8-runtime-ubuntu22.04
        Command:
          - bash
          - -c
          - |
            echo "Starting GPU Benchmark"
            nvidia-smi
            
            # Install Python and necessary libraries
            apt-get update && apt-get install -y python3-pip libgl1-mesa-glx libglib2.0-0
            pip3 install numpy pillow opencv-python torch torchvision
            
            # Create benchmark script
            cat << 'EOF' > /tmp/benchmark.py
            import time
            import numpy as np
            import torch
            import cv2
            import os
            import gc
            
            def benchmark_gpu():
                print("=== GPU Benchmark for Image Processing ===")
                print("CUDA available:", torch.cuda.is_available())
                if torch.cuda.is_available():
                    print(f"GPU Device: {torch.cuda.get_device_name(0)}")
                    print(f"CUDA Version: {torch.version.cuda}")
                    print(f"CUDA Capability: {torch.cuda.get_device_capability(0)}")
                    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
                    
                    # 1. Matrix Operations Benchmark
                    print("\n=== 1. Running Matrix Operations Benchmark ===")
                    sizes = [1000, 2000, 5000]
                    for size in sizes:
                        # Clear GPU memory before each test
                        torch.cuda.empty_cache()
                        gc.collect()
                        
                        start = time.time()
                        
                        # Create large tensors on GPU
                        a = torch.randn(size, size, device='cuda')
                        b = torch.randn(size, size, device='cuda')
                        
                        # Matrix multiplication
                        c = torch.matmul(a, b)
                        torch.cuda.synchronize()  # Wait for operations to complete
                        
                        end = time.time()
                        print(f"Matrix multiplication {size}x{size}: {end - start:.2f} seconds")
                        
                        # Free memory
                        del a, b, c
                        torch.cuda.empty_cache()
                        gc.collect()
                    
                    # 2. Image Processing Benchmark
                    print("\n=== 2. Running Image Processing Benchmark ===")
                    img_sizes = [2048, 4096]
                    batch_sizes = [1, 2, 4]
                    
                    for img_size in img_sizes:
                        print(f"\nProcessing images of size {img_size}x{img_size}")
                        # Generate synthetic image
                        img = np.random.randint(0, 256, (img_size, img_size, 3), dtype=np.uint8)
                        
                        for batch_size in batch_sizes:
                            # Clear GPU memory before each test
                            torch.cuda.empty_cache()
                            gc.collect()
                            
                            # Convert to torch tensor on GPU
                            start = time.time()
                            img_tensor = torch.from_numpy(img).permute(2, 0, 1).float().cuda() / 255.0
                            
                            # Create batch
                            batch = img_tensor.unsqueeze(0).repeat(batch_size, 1, 1, 1)
                            
                            # Image processing operations
                            
                            # Convolutions
                            conv1 = torch.nn.Conv2d(3, 16, kernel_size=3, padding=1).cuda()
                            conv2 = torch.nn.Conv2d(16, 32, kernel_size=3, padding=1).cuda()
                            out = conv1(batch)
                            out = torch.nn.functional.relu(out)
                            out = conv2(out)
                            out = torch.nn.functional.relu(out)
                            
                            # Pooling
                            out = torch.nn.functional.max_pool2d(out, 2)
                            
                            # More convolutions
                            conv3 = torch.nn.Conv2d(32, 64, kernel_size=3, padding=1).cuda()
                            out = conv3(out)
                            out = torch.nn.functional.relu(out)
                            
                            # Batch normalization
                            bn = torch.nn.BatchNorm2d(64).cuda()
                            out = bn(out)
                            
                            # Final pooling
                            out = torch.nn.functional.max_pool2d(out, 2)
                            
                            torch.cuda.synchronize()  # Wait for operations to complete
                            
                            end = time.time()
                            print(f"Batch size {batch_size}: {end - start:.2f} seconds")
                            
                            # Free memory
                            del img_tensor, batch, out, conv1, conv2, conv3, bn
                            torch.cuda.empty_cache()
                            gc.collect()
                    
                    # 3. Advanced Image Transformations
                    print("\n=== 3. Running Advanced Image Transformations ===")
                    img_size = 2048
                    img = np.random.randint(0, 256, (img_size, img_size, 3), dtype=np.uint8)
                    
                    # Clear GPU memory before test
                    torch.cuda.empty_cache()
                    gc.collect()
                    
                    img_tensor = torch.from_numpy(img).permute(2, 0, 1).float().cuda() / 255.0
                    
                    # Record execution times for various operations
                    operations = {}
                    
                    # Gaussian Blur
                    start = time.time()
                    kernel_size = 15
                    sigma = 5.0
                    channels = img_tensor.shape[0]
                    
                    # Create 2D Gaussian kernel
                    kernel_size = kernel_size - kernel_size % 2  # Ensure odd size
                    kernel_x = torch.arange(kernel_size, device='cuda') - kernel_size // 2
                    kernel = torch.exp(-(kernel_x.view(-1, 1) ** 2 + kernel_x.view(1, -1) ** 2) / (2 * sigma ** 2))
                    kernel = kernel / kernel.sum()
                    
                    # Apply convolution for each channel
                    kernel = kernel.view(1, 1, kernel_size, kernel_size)
                    kernel = kernel.repeat(channels, 1, 1, 1)
                    padded = torch.nn.functional.pad(img_tensor.unsqueeze(0), (kernel_size//2, kernel_size//2, kernel_size//2, kernel_size//2), mode='reflect')
                    blurred = torch.nn.functional.conv2d(padded, kernel, groups=channels)
                    torch.cuda.synchronize()
                    operations["Gaussian Blur"] = time.time() - start
                    
                    # Edge Detection (Sobel)
                    start = time.time()
                    sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32, device='cuda').view(1, 1, 3, 3)
                    sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32, device='cuda').view(1, 1, 3, 3)
                    
                    padded = torch.nn.functional.pad(blurred, (1, 1, 1, 1), mode='reflect')
                    g_x = torch.nn.functional.conv2d(padded, sobel_x.repeat(channels, 1, 1, 1), groups=channels)
                    g_y = torch.nn.functional.conv2d(padded, sobel_y.repeat(channels, 1, 1, 1), groups=channels)
                    edges = torch.sqrt(g_x**2 + g_y**2)
                    torch.cuda.synchronize()
                    operations["Edge Detection"] = time.time() - start
                    
                    # Print results
                    print("\nAdvanced Operations Timing:")
                    for op, time_taken in operations.items():
                        print(f"{op}: {time_taken:.2f} seconds")
                    
                    # Free memory
                    del img_tensor, kernel, padded, blurred, sobel_x, sobel_y, g_x, g_y, edges
                    torch.cuda.empty_cache()
                    
                    print("\n=== GPU Benchmark Complete ===")
                else:
                    print("CUDA is not available. Cannot run GPU benchmark.")
            
            if __name__ == "__main__":
                benchmark_gpu()
            EOF
            
            # Run the benchmark
            python3 /tmp/benchmark.py
        Vcpus: 32             # Increased from 16 to 32 for larger instances
        Memory: 122880        # Increased from 61440 to 122880 (120GB)
        ResourceRequirements:
          - Type: GPU
            Value: "1"
        JobRoleArn: !GetAtt JobRole.Arn
        LinuxParameters:
          Devices:
            - HostPath: /dev/nvidia0
              ContainerPath: /dev/nvidia0
              Permissions:
                - READ
                - WRITE

  CPUImageProcessingJobDefinition:
    Type: AWS::Batch::JobDefinition
    Properties:
      Type: container
      JobDefinitionName: cpu-image-processing-benchmark
      Timeout:
        AttemptDurationSeconds: 43200  # 12 hours - CPUs may need more time
      ContainerProperties:
        Image: nvidia/cuda:12.0.1-cudnn8-runtime-ubuntu22.04  # Using the same image as GPU for consistency
        Command:
          - bash
          - -c
          - |
            echo "Starting CPU Benchmark"
            
            # Install Python and necessary libraries
            apt-get update && apt-get install -y python3-pip libgl1-mesa-glx libglib2.0-0
            pip3 install numpy pillow opencv-python torch torchvision
            
            # Create benchmark script
            cat << 'EOF' > /tmp/benchmark.py
            import time
            import numpy as np
            import torch
            import cv2
            import os
            import gc
            
            def benchmark_cpu():
                print("=== CPU Benchmark for Image Processing ===")
                
                # Force CPU computations
                torch.set_num_threads(os.cpu_count())
                device = torch.device('cpu')
                print(f"Using CPU with {os.cpu_count()} threads")
                print(f"PyTorch version: {torch.__version__}")
                
                # 1. Matrix Operations Benchmark
                print("\n=== 1. Running Matrix Operations Benchmark ===")
                sizes = [1000, 2000, 5000]  # Same sizes as GPU benchmark
                for size in sizes:
                    # Clear memory before each test
                    gc.collect()
                    
                    start = time.time()
                    
                    # Create large tensors on CPU
                    a = torch.randn(size, size, device=device)
                    b = torch.randn(size, size, device=device)
                    
                    # Matrix multiplication
                    c = torch.matmul(a, b)
                    
                    end = time.time()
                    print(f"Matrix multiplication {size}x{size}: {end - start:.2f} seconds")
                    
                    # Free memory
                    del a, b, c
                    gc.collect()
                
                # 2. Image Processing Benchmark
                print("\n=== 2. Running Image Processing Benchmark ===")
                # Now we can use the same sizes as GPU since we have more memory
                img_sizes = [2048, 4096]  # Same as GPU benchmark
                batch_sizes = [1, 2, 4]   # Same as GPU benchmark
                
                for img_size in img_sizes:
                    print(f"\nProcessing images of size {img_size}x{img_size}")
                    # Generate synthetic image
                    img = np.random.randint(0, 256, (img_size, img_size, 3), dtype=np.uint8)
                    
                    for batch_size in batch_sizes:
                        # Clear memory before each test
                        gc.collect()
                        
                        try:
                            # Convert to torch tensor on CPU
                            start = time.time()
                            img_tensor = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0
                            
                            # Create batch
                            batch = img_tensor.unsqueeze(0).repeat(batch_size, 1, 1, 1)
                            
                            # Image processing operations
                            
                            # Convolutions
                            conv1 = torch.nn.Conv2d(3, 16, kernel_size=3, padding=1)
                            conv2 = torch.nn.Conv2d(16, 32, kernel_size=3, padding=1)
                            out = conv1(batch)
                            out = torch.nn.functional.relu(out)
                            out = conv2(out)
                            out = torch.nn.functional.relu(out)
                            
                            # Pooling
                            out = torch.nn.functional.max_pool2d(out, 2)
                            
                            # More convolutions
                            conv3 = torch.nn.Conv2d(32, 64, kernel_size=3, padding=1)
                            out = conv3(out)
                            out = torch.nn.functional.relu(out)
                            
                            # Batch normalization
                            bn = torch.nn.BatchNorm2d(64)
                            out = bn(out)
                            
                            # Final pooling
                            out = torch.nn.functional.max_pool2d(out, 2)
                            
                            end = time.time()
                            print(f"Batch size {batch_size}: {end - start:.2f} seconds")
                            
                            # Free memory
                            del img_tensor, batch, out, conv1, conv2, conv3, bn
                            gc.collect()
                        except RuntimeError as e:
                            if "out of memory" in str(e):
                                print(f"Not enough memory for image size {img_size} with batch size {batch_size}")
                            else:
                                print(f"Error processing image size {img_size} with batch size {batch_size}: {e}")
                
                # 3. Advanced Image Transformations
                print("\n=== 3. Running Advanced Image Transformations ===")
                img_size = 2048  # Same size as GPU benchmark
                img = np.random.randint(0, 256, (img_size, img_size, 3), dtype=np.uint8)
                img_tensor = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0
                
                # Record execution times for various operations
                operations = {}
                
                # Gaussian Blur
                start = time.time()
                kernel_size = 15
                sigma = 5.0
                channels = img_tensor.shape[0]
                
                # Create 2D Gaussian kernel
                kernel_size = kernel_size - kernel_size % 2  # Ensure odd size
                kernel_x = torch.arange(kernel_size) - kernel_size // 2
                kernel = torch.exp(-(kernel_x.view(-1, 1) ** 2 + kernel_x.view(1, -1) ** 2) / (2 * sigma ** 2))
                kernel = kernel / kernel.sum()
                
                # Apply convolution for each channel
                kernel = kernel.view(1, 1, kernel_size, kernel_size)
                kernel = kernel.repeat(channels, 1, 1, 1)
                padded = torch.nn.functional.pad(img_tensor.unsqueeze(0), (kernel_size//2, kernel_size//2, kernel_size//2, kernel_size//2), mode='reflect')
                blurred = torch.nn.functional.conv2d(padded, kernel, groups=channels)
                
                operations["Gaussian Blur"] = time.time() - start
                
                # Edge Detection (Sobel)
                start = time.time()
                sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3)
                sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3)
                
                padded = torch.nn.functional.pad(blurred, (1, 1, 1, 1), mode='reflect')
                g_x = torch.nn.functional.conv2d(padded, sobel_x.repeat(channels, 1, 1, 1), groups=channels)
                g_y = torch.nn.functional.conv2d(padded, sobel_y.repeat(channels, 1, 1, 1), groups=channels)
                edges = torch.sqrt(g_x**2 + g_y**2)
                
                operations["Edge Detection"] = time.time() - start
                
                # Print results
                print("\nAdvanced Operations Timing:")
                for op, time_taken in operations.items():
                    print(f"{op}: {time_taken:.2f} seconds")
                
                print("\n=== CPU Benchmark Complete ===")
            
            if __name__ == "__main__":
                benchmark_cpu()
            EOF
            
            # Run the benchmark
            python3 /tmp/benchmark.py
        Vcpus: 32             # Increased to match GPU job definition
        Memory: 122880        # Increased to match GPU job definition (120GB)
        JobRoleArn: !GetAtt JobRole.Arn

  # CloudWatch Logs Group
  BatchLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: /aws/batch/gpu-benchmark
      RetentionInDays: 14

Outputs:
  GpuJobQueue:
    Description: ARN of the GPU Job Queue
    Value: !Ref GpuJobQueue
    
  CpuJobQueue:
    Description: ARN of the CPU Job Queue
    Value: !Ref CPUJobQueue
    
  GpuJobDefinition:
    Description: ARN of the GPU Job Definition
    Value: !Ref ImageProcessingJobDefinition

  CpuJobDefinition:
    Description: ARN of the CPU Job Definition
    Value: !Ref CPUImageProcessingJobDefinition
    
  SubmitGpuJobCommand:
    Description: Command to submit a GPU job
    Value: !Sub "aws batch submit-job --job-name gpu-benchmark-$(date +%Y%m%d%H%M%S) --job-queue ${GpuJobQueue} --job-definition ${ImageProcessingJobDefinition}"

  SubmitCpuJobCommand:
    Description: Command to submit a CPU job
    Value: !Sub "aws batch submit-job --job-name cpu-benchmark-$(date +%Y%m%d%H%M%S) --job-queue ${CPUJobQueue} --job-definition ${CPUImageProcessingJobDefinition}"

  ListRunningGpuJobsCommand:
    Description: Command to list running GPU jobs
    Value: !Sub "aws batch list-jobs --job-queue ${GpuJobQueue} --job-status RUNNING"

  ListRunningCpuJobsCommand:
    Description: Command to list running CPU jobs
    Value: !Sub "aws batch list-jobs --job-queue ${CPUJobQueue} --job-status RUNNING"

  DescribeJobsCommand:
    Description: Command to describe specific jobs (replace job-id-1 with actual job IDs)
    Value: "aws batch describe-jobs --jobs job-id-1 job-id-2"

  GetGpuJobLogsCommand:
    Description: Command to get logs for a GPU job (replace job-id with actual ID)
    Value: !Sub "LOG_STREAM=$(aws batch describe-jobs --jobs job-id --query 'jobs[0].container.logStreamName' --output text) && aws logs get-log-events --log-group-name /aws/batch/gpu-benchmark --log-stream-name $LOG_STREAM"

  GetCpuJobLogsCommand:
    Description: Command to get logs for a CPU job (replace job-id with actual ID)
    Value: !Sub "LOG_STREAM=$(aws batch describe-jobs --jobs job-id --query 'jobs[0].container.logStreamName' --output text) && aws logs get-log-events --log-group-name /aws/batch/gpu-benchmark --log-stream-name $LOG_STREAM"

  GetBenchmarkResultsCommand:
    Description: Command to extract just the benchmark timing results (replace job-id with actual ID)
    Value: !Sub "LOG_STREAM=$(aws batch describe-jobs --jobs job-id --query 'jobs[0].container.logStreamName' --output text) && aws logs get-log-events --log-group-name /aws/batch/gpu-benchmark --log-stream-name $LOG_STREAM --output text | grep -E 'multiplication|Batch size|Operation'"

  MonitorJobStatusCommand:
    Description: Command to monitor job status (replace job-id with actual ID)
    Value: "watch -n 10 'aws batch describe-jobs --jobs job-id --query \"jobs[0].{JobName:jobName,Status:status,Reason:statusReason,StartedAt:startedAt,StoppedAt:stoppedAt}\"'"

  VpcId:
    Description: ID of the VPC created for the Batch environment
    Value: !Ref BatchVPC
    
  SubnetIds:
    Description: IDs of the subnets created for the Batch environment
    Value: !Join [", ", [!Ref PublicSubnet1, !Ref PublicSubnet2]]
